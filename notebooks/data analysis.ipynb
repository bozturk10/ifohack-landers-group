{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.sklearn import log_model\n",
    "import os\n",
    "\n",
    "raw_data_dir= os.path.join(\"..\",'data','raw')\n",
    "land_price_dir= os.path.join(raw_data_dir,'1 Land Prices') #\n",
    "koeln_landprice_gpkg=os.path.join(land_price_dir,'Land_Prices_Neighborhood_Köln.gpkg') \n",
    "# Read in the two datasets\n",
    "neighborhoods = gpd.read_file(koeln_landprice_gpkg)\n",
    "grid = gpd.read_file('Zensus_Köln_Grid_100m.gpkg')\n",
    "\n",
    "# Perform spatial join using sjoin\n",
    "merged_data = gpd.sjoin(grid, neighborhoods, how='left', op='intersects')\n",
    "\n",
    "# Save merged data to file\n",
    "merged_data.to_file('merged_data.gpkg', driver='GPKG')\n",
    "\n",
    "\n",
    "# Read in merged data\n",
    "merged_data = gpd.read_file(\"merged_data.gpkg\")\n",
    "\n",
    "# Read in Sentinel data\n",
    "with rasterio.open(\"Sentinel_Köln.tif\") as src:\n",
    "    sentinel_data = src.read(1)\n",
    "    sentinel_transform = src.transform\n",
    "\n",
    "# Read in WorldCover data\n",
    "with rasterio.open(\"WorldCover_Köln.tif\") as src:\n",
    "    worldcover_data = src.read(1)\n",
    "    worldcover_transform = src.transform\n",
    "\n",
    "# Compute average population density in the neighborhood\n",
    "merged_data[\"population_density\"] = merged_data[\"Population\"] / merged_data.geometry.area\n",
    "\n",
    "# Compute amount of green space in the neighborhood\n",
    "merged_data[\"green_space\"] = np.sum(worldcover_data[merged_data.geometry.to_crs(src.crs).bounds.round().astype(int)])\n",
    "\n",
    "# Compute number of buildings with special function in the neighborhood\n",
    "special_functions = [\"school\", \"hospital\", \"restaurant\"]\n",
    "for sf in special_functions:\n",
    "    buildings = gpd.read_file(f\"OSM_{sf}_Köln.gpkg\")\n",
    "    buildings_in_neighborhoods = gpd.sjoin(merged_data, buildings, op='intersects')\n",
    "    merged_data[f\"{sf}_buildings\"] = buildings_in_neighborhoods.groupby(\"Neighborhood_FID\").size()\n",
    "\n",
    "# Compute average (or shortest) distance from each residential building to a building with a special function\n",
    "residential_buildings = gpd.read_file(\"OSM_residential_Köln.gpkg\")\n",
    "for sf in special_functions:\n",
    "    buildings = gpd.read_file(f\"OSM_{sf}_Köln.gpkg\")\n",
    "    distances = []\n",
    "    for index, residential_building in residential_buildings.iterrows():\n",
    "        distances_to_buildings = buildings.distance(residential_building.geometry)\n",
    "        distances.append(np.min(distances_to_buildings))\n",
    "    merged_data[f\"{sf}_distance\"] = distances\n",
    "\n",
    "# Compute total length of walkable paths in the neighborhood\n",
    "walkable_paths = gpd.read_file(\"OSM_walkable_paths_Köln.gpkg\")\n",
    "paths_in_neighborhoods = gpd.sjoin(merged_data, walkable_paths, op='intersects')\n",
    "merged_data[\"walkable_path_length\"] = paths_in_neighborhoods.groupby(\"Neighborhood_FID\").length.sum()\n",
    "\n",
    "# Compute isolation of the senior population\n",
    "senior_population = merged_data[\"Population_65+\"] / merged_data[\"Population\"]\n",
    "merged_data[\"isolation_seniors\"] = senior_population - np.mean(senior_population)\n",
    "\n",
    "# Prepare data for regression\n",
    "X = merged_data[[\"population_density\", \"green_space\", \"school_buildings\", \"hospital_buildings\",\n",
    "                 \"restaurant_buildings\", \"school_distance\", \"hospital_distance\", \"restaurant_distance\",\n",
    "                 \"walkable_path_length\", \"isolation_seniors\"]]\n",
    "y = merged_data[\"Land_Value\"]\n",
    "\n",
    "# Scale X data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Log model and parameters with MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model_type\", \"linear_regression\")\n",
    "    mlflow.log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BerkÖztürk\\AppData\\Local\\Temp\\ipykernel_15596\\1218111049.py:1: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.sklearn import log_model\n",
    "import os\n",
    "\n",
    "raw_data_dir= os.path.join(\"..\",'data','raw')\n",
    "land_price_dir= os.path.join(raw_data_dir,'1 Land Prices') #\n",
    "koeln_landprice_gpkg=os.path.join(land_price_dir,'Land_Prices_Neighborhood_Köln.gpkg') \n",
    "# Read in the two datasets\n",
    "neighborhoods = gpd.read_file(koeln_landprice_gpkg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifohack_spatial_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
