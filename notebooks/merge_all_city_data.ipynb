{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Landprices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BerkÖztürk\\AppData\\Local\\Temp\\ipykernel_7964\\1734800707.py:2: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector data \n",
    "We start by reading and quickly visualising vector data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raster data\n",
    "\n",
    "We read the census data that is classified in a grid, then we read the grid as a geopandas file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZENSUS_PATH = r\"../data/raw/2 Zensus/\"\n",
    "NEIGHBORHOOD_PATH =  r\"../data/raw/3 Neighborhoods\"\n",
    "LAND_PRICES_PATH = r\"../data/raw/1 Land Prices\"\n",
    "CITY_NAMES=[\"Berlin\",\"Bremen\",\"Dresden\",\"Frankfurt\",\"Köln\"]\n",
    "zensus_files= os.listdir(ZENSUS_PATH)\n",
    "neighborhood_files= os.listdir(NEIGHBORHOOD_PATH)\n",
    "landprices_files= os.listdir(LAND_PRICES_PATH)\n",
    "\n",
    "def get_file_names(): \n",
    "    city_files={}\n",
    "    for city_name in CITY_NAMES:\n",
    "        \n",
    "        csv_fpaths= [os.path.join(ZENSUS_PATH,fpath) for fpath in zensus_files if ( city_name in fpath ) and (fpath.endswith(\".csv\")) ]\n",
    "        gpkg_fpaths= [os.path.join(ZENSUS_PATH,fpath) for fpath in zensus_files if ( city_name in fpath ) and (fpath.endswith(\".gpkg\")) ]\n",
    "        \n",
    "        neighbourhood_fpaths= [os.path.join(NEIGHBORHOOD_PATH,fpath) for fpath in neighborhood_files if ( city_name in fpath ) and (fpath.endswith(\".gpkg\")) ]\n",
    "        landprices_fpaths= [os.path.join(LAND_PRICES_PATH,fpath) for fpath in landprices_files if ( city_name in fpath ) ]\n",
    "\n",
    "        city_files[city_name]= (csv_fpaths,gpkg_fpaths,neighbourhood_fpaths,landprices_fpaths)\n",
    "    return city_files\n",
    "\n",
    "def combine_data_within_city(city_files):\n",
    "    city_merged_data={}\n",
    "    for city_name, (csv_fpaths,gpkg_fpaths,neighbourhood_fpaths,landprices_fpaths) in city_files.items():\n",
    "        df_list_city=[]\n",
    "        for csv_fpath in csv_fpaths:\n",
    "            df = pd.read_csv(csv_fpath, sep=\";\", encoding=\"utf-8-sig\").drop(columns=\"Unnamed: 0\")\n",
    "            df_list_city.append(df)\n",
    "\n",
    "        from functools import reduce\n",
    "        zensus_csv_merged = reduce(lambda df1,df2: pd.merge(df1,df2,on=\"Grid_Code\"), df_list_city)\n",
    "        \n",
    "        # read grid\n",
    "        grid_city = gpd.read_file(gpkg_fpaths[0])\n",
    "        grid_city = grid_city.merge(zensus_csv_merged, on = \"Grid_Code\")\n",
    "\n",
    "        # merge files\n",
    "        prices_city = pd.read_csv(landprices_fpaths[0], sep=\";\", encoding= \"utf-8-sig\").drop(columns=\"Unnamed: 0\")\n",
    "        neighborhood_city = gpd.read_file(neighbourhood_fpaths[0])\n",
    "        neighborhood_city = neighborhood_city.merge(prices_city, on = \"Neighborhood_FID\", how = \"inner\")\n",
    "\n",
    "        neighborhood_city_4326 = neighborhood_city.to_crs(epsg = 4326)\n",
    "        amenity_features = get_amenity_features(neighborhood_city_4326)\n",
    "\n",
    "        # Perform spatial join using sjoin\n",
    "        merged_data = gpd.sjoin(grid_city, neighborhood_city, how='left', op='intersects')\n",
    "        merged_data = merged_data.drop(columns=[\"City_Name_y\", \"City_Code_right\"]).rename(columns={\"City_Name_x\":\"City_Name\", \"City_Code_left\":\"City_Code\"})\n",
    "        city_merged_data[city_name] = merged_data\n",
    "    return city_merged_data\n",
    "\n",
    "def concat_city_dataframes(city_merged_data):\n",
    "    col_list=[]\n",
    "    for city_name,df in city_merged_data.items():\n",
    "        print(df.shape)\n",
    "        col_list.append(df.columns.to_list())\n",
    "    common_cols=list(set(col_list[0]).intersection(*col_list))\n",
    "\n",
    "    grid_level_df_list = [df[common_cols] for df in city_merged_data.values()]\n",
    "    grid_level_all_cities = pd.concat(grid_level_df_list, axis=0)\n",
    "    return grid_level_all_cities\n",
    "\n",
    "def get_amenity_features(neighborhoods):\n",
    "    # Define the tag to extract (amenity=restaurant)\n",
    "    tag = {'amenity':True}\n",
    "\n",
    "    # Define an empty list to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Loop over each neighborhood and extract the restaurants\n",
    "    for i, nb_name in enumerate(neighborhoods.Neighborhood_Name):\n",
    "        nb = neighborhoods.loc[neighborhoods.Neighborhood_Name == nb_name]\n",
    "        restaurants = ox.geometries.geometries_from_polygon(polygon=nb.geometry.iloc[0], tags=tag)\n",
    "        # print(f'Processed {i+1}/{len(neighborhoods)} neighborhoods ({nb_name}): found {restaurants.shape[0]} restaurants')\n",
    "        results[nb_name] = restaurants\n",
    "\n",
    "    nb_results = []\n",
    "\n",
    "    for i, nb_name in enumerate(neighborhoods.Neighborhood_Name):\n",
    "        nb_result = pd.DataFrame(results[nb_name].amenity.value_counts().to_dict(), index=[i])\n",
    "        nb_result[\"Neighborhood_Name\"]=nb_name\n",
    "        nb_result[\"City_Name\"]=neighborhoods.City_Name\n",
    "        nb_results.append(nb_result)\n",
    "\n",
    "    # Combine the results into a single GeoDataFrame\n",
    "    combined_results = pd.concat(nb_results, ignore_index=True)\n",
    "    \n",
    "\n",
    "    DROPNA_TRESH=0.8\n",
    "    combined_results = combined_results.loc[:, df.isnull().mean() < DROPNA_TRESH]\n",
    "\n",
    "    return combined_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_files= get_file_names()\n",
    "\n",
    "\n",
    "city_merged_data = combine_data_within_city(city_files)\n",
    "\n",
    "grid_level_all_cities= concat_city_dataframes(city_merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_area_names=pd.Series(grid_level_all_cities.Area_Types.unique()).str.split(\"_\")\n",
    "unique_area_cols = pd.Series(np.concatenate(splitted_area_names)).unique().tolist()\n",
    "for unique_area_col in unique_area_cols:\n",
    "    grid_level_all_cities['is_{}'.format(unique_area_col)]= grid_level_all_cities.Area_Types.str.contains(unique_area_col).astype(int)\n",
    "grid_level_all_cities = grid_level_all_cities.drop(columns='Area_Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "numeric_cols = grid_level_all_cities.select_dtypes(include=['int32','int64']).columns\n",
    "non_numeric_cols = grid_level_all_cities.select_dtypes(exclude=['int32','int64']).columns\n",
    "\n",
    "agg_operations= dict(zip(numeric_cols, ['mean']*len(numeric_cols) ))\n",
    "agg_operations[\"Land_Value\"]=\"first\"\n",
    "grid_level_all_cities_neighborhood_level =  grid_level_all_cities.groupby(['City_Name','Neighborhood_Name']).agg(agg_operations).reset_index()\n",
    "#grid_level_all_cities_neighborhood_level= grid_level_all_cities_neighborhood_level.drop(columns='index_right')#.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_level_all_cities_neighborhood_level.to_csv('../data/interim/nb_level_merged_all_cities.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save merged data to file\n",
    "grid_level_all_cities.to_file('../data/interim/grid_level_merged_all_cities.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        neighborhood_city = gpd.read_file(neighbourhood_fpaths[0])\n",
    "        neighborhood_city = neighborhood_city.merge(prices_city, on = \"Neighborhood_FID\", how = \"inner\")\n",
    "\n",
    "        neighborhood_city_4326 = neighborhood_city.to_crs(epsg = 4326)\n",
    "        amenity_features = get_amenity_features(neighborhood_city_4326)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ifohack_spatial_py310_small_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
